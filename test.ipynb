{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True\n",
      "Number of GPUs:  1\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "print(\"CUDA Available: \", torch.cuda.is_available())\n",
    "print(\"Number of GPUs: \", torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU 0: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "# Print GPU details\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple tensor computation\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "a = torch.randn((1000, 1000), device=device)\n",
    "b = torch.randn((1000, 1000), device=device)\n",
    "c = torch.matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation complete\n",
      "Memory Allocated on GPU 0: 20971520 bytes\n",
      "Memory Cached on GPU 0: 20971520 bytes\n"
     ]
    }
   ],
   "source": [
    "print(\"Computation complete\")\n",
    "\n",
    "# Check GPU memory usage\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"Memory Allocated on GPU {i}: {torch.cuda.memory_allocated(i)} bytes\")\n",
    "    print(f\"Memory Cached on GPU {i}: {torch.cuda.memory_reserved(i)} bytes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available:  True\n",
      "Number of GPUs:  1\n",
      "GPU 0: NVIDIA GeForce GTX 1650\n",
      "Computation on GPU 0 complete\n",
      "Memory Allocated on GPU 0: 32972288 bytes\n",
      "Memory Cached on GPU 0: 41943040 bytes\n",
      "GPU 1 not available\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "print(\"CUDA Available: \", torch.cuda.is_available())\n",
    "print(\"Number of GPUs: \", torch.cuda.device_count())\n",
    "\n",
    "# Print GPU details\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "\n",
    "# Specify the device to use (GPU 0 and GPU 1)\n",
    "device0 = torch.device('cuda:0' if torch.cuda.device_count() > 0 else 'cpu')\n",
    "device1 = torch.device('cuda:1' if torch.cuda.device_count() > 1 else 'cpu')\n",
    "\n",
    "# Create a simple tensor computation on GPU 0\n",
    "a0 = torch.randn((1000, 1000), device=device0)\n",
    "b0 = torch.randn((1000, 1000), device=device0)\n",
    "c0 = torch.matmul(a0, b0)\n",
    "print(\"Computation on GPU 0 complete\")\n",
    "\n",
    "# Check GPU 0 memory usage\n",
    "print(f\"Memory Allocated on GPU 0: {torch.cuda.memory_allocated(0)} bytes\")\n",
    "print(f\"Memory Cached on GPU 0: {torch.cuda.memory_reserved(0)} bytes\")\n",
    "\n",
    "# Create a simple tensor computation on GPU 1 (if available)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    a1 = torch.randn((1000, 1000), device=device1)\n",
    "    b1 = torch.randn((1000, 1000), device=device1)\n",
    "    c1 = torch.matmul(a1, b1)\n",
    "    print(\"Computation on GPU 1 complete\")\n",
    "\n",
    "    # Check GPU 1 memory usage\n",
    "    print(f\"Memory Allocated on GPU 1: {torch.cuda.memory_allocated(1)} bytes\")\n",
    "    print(f\"Memory Cached on GPU 1: {torch.cuda.memory_reserved(1)} bytes\")\n",
    "else:\n",
    "    print(\"GPU 1 not available\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-directml\n",
      "  Using cached torch_directml-0.2.3.dev240715-cp38-cp38-win_amd64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: torch==2.3.1 in c:\\users\\chari\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch-directml) (2.3.1+cu118)\n",
      "Requirement already satisfied: torchvision==0.18.1 in c:\\users\\chari\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch-directml) (0.18.1+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\chari\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch==2.3.1->torch-directml) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\chari\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch==2.3.1->torch-directml) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\chari\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch==2.3.1->torch-directml) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\chari\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch==2.3.1->torch-directml) (3.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\chari\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch==2.3.1->torch-directml) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\chari\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch==2.3.1->torch-directml) (2024.2.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\chari\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch==2.3.1->torch-directml) (2021.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\chari\\anaconda3\\envs\\myenv\\lib\\site-packages (from torchvision==0.18.1->torch-directml) (1.24.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\chari\\anaconda3\\envs\\myenv\\lib\\site-packages (from torchvision==0.18.1->torch-directml) (10.2.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\chari\\anaconda3\\envs\\myenv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch==2.3.1->torch-directml) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\chari\\anaconda3\\envs\\myenv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch==2.3.1->torch-directml) (2021.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\chari\\anaconda3\\envs\\myenv\\lib\\site-packages (from jinja2->torch==2.3.1->torch-directml) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\chari\\anaconda3\\envs\\myenv\\lib\\site-packages (from sympy->torch==2.3.1->torch-directml) (1.3.0)\n",
      "Downloading torch_directml-0.2.3.dev240715-cp38-cp38-win_amd64.whl (9.0 MB)\n",
      "   ---------------------------------------- 0.0/9.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.0 MB 262.6 kB/s eta 0:00:34\n",
      "   ---------------------------------------- 0.1/9.0 MB 508.4 kB/s eta 0:00:18\n",
      "    --------------------------------------- 0.2/9.0 MB 817.0 kB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.3/9.0 MB 1.3 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.3/9.0 MB 1.2 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.3/9.0 MB 1.1 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 0.4/9.0 MB 1.1 MB/s eta 0:00:08\n",
      "   - -------------------------------------- 0.4/9.0 MB 1.1 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 0.5/9.0 MB 994.6 kB/s eta 0:00:09\n",
      "   -- ------------------------------------- 0.5/9.0 MB 1.0 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 0.5/9.0 MB 1.0 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 0.6/9.0 MB 956.7 kB/s eta 0:00:09\n",
      "   -- ------------------------------------- 0.6/9.0 MB 942.5 kB/s eta 0:00:09\n",
      "   -- ------------------------------------- 0.7/9.0 MB 937.7 kB/s eta 0:00:09\n",
      "   -- ------------------------------------- 0.7/9.0 MB 937.7 kB/s eta 0:00:09\n",
      "   --- ------------------------------------ 0.7/9.0 MB 868.9 kB/s eta 0:00:10\n",
      "   --- ------------------------------------ 0.7/9.0 MB 878.1 kB/s eta 0:00:10\n",
      "   --- ------------------------------------ 0.8/9.0 MB 924.5 kB/s eta 0:00:09\n",
      "   --- ------------------------------------ 0.9/9.0 MB 955.1 kB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 0.9/9.0 MB 915.8 kB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 1.1/9.0 MB 1.1 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 1.3/9.0 MB 1.2 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 1.5/9.0 MB 1.3 MB/s eta 0:00:06\n",
      "   -------- ------------------------------- 1.8/9.0 MB 1.5 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 2.1/9.0 MB 1.7 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 2.5/9.0 MB 2.0 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 2.9/9.0 MB 2.1 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.2/9.0 MB 2.3 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 3.6/9.0 MB 2.5 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 3.8/9.0 MB 2.5 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 4.1/9.0 MB 2.7 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.5/9.0 MB 2.8 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.9/9.0 MB 2.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 5.3/9.0 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.6/9.0 MB 3.2 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 6.0/9.0 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.4/9.0 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.7/9.0 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.1/9.0 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.3/9.0 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.6/9.0 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.1/9.0 MB 3.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.3/9.0 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.5/9.0 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.9/9.0 MB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.0/9.0 MB 3.9 MB/s eta 0:00:00\n",
      "Installing collected packages: torch-directml\n",
      "Successfully installed torch-directml-0.2.3.dev240715\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch-directml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation on DirectML (AMD GPU) complete\n",
      "Computation on CUDA (NVIDIA GPU) complete\n"
     ]
    }
   ],
   "source": [
    "import torch_directml\n",
    "\n",
    "# Initialize DirectML devices\n",
    "device0 = torch_directml.device()\n",
    "device1 = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create a simple tensor computation on DirectML device (AMD GPU)\n",
    "a0 = torch.randn((1000, 1000), device=device0)\n",
    "b0 = torch.randn((1000, 1000), device=device0)\n",
    "c0 = torch.matmul(a0, b0)\n",
    "print(\"Computation on DirectML (AMD GPU) complete\")\n",
    "\n",
    "# Create a simple tensor computation on CUDA device (NVIDIA GPU)\n",
    "a1 = torch.randn((1000, 1000), device=device1)\n",
    "b1 = torch.randn((1000, 1000), device=device1)\n",
    "c1 = torch.matmul(a1, b1)\n",
    "print(\"Computation on CUDA (NVIDIA GPU) complete\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
